{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import math\n",
    "from torch import Tensor\n",
    "# import os\n",
    "# from utils.tools import EarlyStopping, adjust_learning_rate\n",
    "import time\n",
    "import numpy as np\n",
    "from data_provider.data_factory import data_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AutoformerConfig:\n",
    "    kernel_size: int        # kernel size for moving average in series decomposition\n",
    "    seq_len: int            # input sequence length\n",
    "    pred_len: int           # forecast horizon\n",
    "    n_encoders: int         # no. of encoder layers\n",
    "    n_decoders: int         # no. of decoder layers\n",
    "    d_model: int            # dimension of model's hidden states and embeddings\n",
    "    n_heads: int            # no. of attention heads\n",
    "    d_ff: int               # dimension of feed-forward network in transformer blocks\n",
    "    \n",
    "    # c: Auto-correlation intensity factor\n",
    "    # Controls the number of time delay steps (k = c * log(L))\n",
    "    # Typically set between 1-3\n",
    "    c: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoCorrelation(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.d_k = config.d_model // config.n_heads\n",
    "        self.c = config.c\n",
    "        \n",
    "        # Projections for Q/K/V\n",
    "        self.query_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.key_proj = nn.Linear(config.d_model, config.d_model) \n",
    "        self.value_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.out_proj = nn.Linear(config.d_model, config.d_model)\n",
    "\n",
    "    def time_delay_agg(self, values: Tensor, corr: Tensor, delays: Tensor) -> Tensor:\n",
    "        # Time delay aggregation\n",
    "        batch_size, L, H, d_k = values.shape\n",
    "        output = torch.zeros_like(values)\n",
    "        \n",
    "        for i, delay in enumerate(delays):\n",
    "            # Roll the values by delay steps\n",
    "            rolled = torch.roll(values, shifts=int(delay), dims=1)\n",
    "            # Weight by correlation score\n",
    "            output += rolled * corr[:, i:i+1, :, None]\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        batch_size, L, _ = q.shape\n",
    "        \n",
    "        # Project and reshape for multi-head\n",
    "        q = self.query_proj(q).view(batch_size, L, self.n_heads, self.d_k)\n",
    "        k = self.key_proj(k).view(batch_size, L, self.n_heads, self.d_k)\n",
    "        v = self.value_proj(v).view(batch_size, L, self.n_heads, self.d_k)\n",
    "\n",
    "        # Compute autocorrelation for each head\n",
    "        fft_q = torch.fft.rfft(q, dim=1)\n",
    "        fft_k = torch.fft.rfft(k, dim=1)\n",
    "        \n",
    "        # Cross correlation in frequency domain\n",
    "        corr = fft_q * torch.conj(fft_k)\n",
    "        corr = torch.fft.irfft(corr, dim=1)\n",
    "        \n",
    "        # Select top-k delays\n",
    "        num_delays = int(self.c * math.log(L))\n",
    "        delays, indices = torch.topk(corr[:, 1:], num_delays, dim=1)  # Exclude 0 lag\n",
    "        delays = F.softmax(delays, dim=-1)\n",
    "        \n",
    "        # Time delay aggregation\n",
    "        out = self.time_delay_agg(v, delays, indices + 1)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        out = out.reshape(batch_size, L, self.d_model)\n",
    "        return self.out_proj(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAvg(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.kernel_size = config.kernel_size\n",
    "        self.padding_size = (self.kernel_size - 1) // 2\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # padding logic taken from original implementation\n",
    "        padding = torch.cat([\n",
    "            x[:, :1, :].repeat(1, self.padding_size, 1),  # front padding\n",
    "            x,\n",
    "            x[:, -1:, :].repeat(1, self.padding_size, 1)  # end padding\n",
    "        ], dim=1)\n",
    "        \n",
    "        # compute moving average using avg_pool1d\n",
    "        return F.avg_pool1d(\n",
    "            padding.permute(0, 2, 1), \n",
    "            kernel_size=self.kernel_size, \n",
    "            stride=1\n",
    "        ).permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeriesDecomp(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super(SeriesDecomp, self).__init__()\n",
    "        self.moving_avg = MovingAvg(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        trend = self.moving_avg(x)\n",
    "        seasonal = x - trend\n",
    "        return seasonal, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.auto_correlation = AutoCorrelation(config)\n",
    "        self.series_decomp = SeriesDecomp(config)\n",
    "        \n",
    "        # Two-layer feed forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.d_ff, config.d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Auto-correlation with residual\n",
    "        auto_out = self.auto_correlation(x, x, x)\n",
    "        s1, _ = self.series_decomp(auto_out + x)\n",
    "        \n",
    "        # Feed forward with residual\n",
    "        ff_out = self.ff(s1)\n",
    "        s2, _ = self.series_decomp(ff_out + s1)\n",
    "        \n",
    "        return s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoformerEncoder(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super(AutoformerEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([AutoformerEncoderLayer(config) for l in range(config.n_encoders)])\n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.auto_correlation = AutoCorrelation(config)\n",
    "        self.cross_correlation = AutoCorrelation(config)\n",
    "        self.series_decomp = SeriesDecomp(config)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.d_ff, config.d_model)\n",
    "        )\n",
    "        \n",
    "        # Trend projection layers\n",
    "        self.trend_proj1 = nn.Linear(config.d_model, config.d_model)\n",
    "        self.trend_proj2 = nn.Linear(config.d_model, config.d_model)\n",
    "        self.trend_proj3 = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "    def forward(self, x_seasonal: Tensor, x_trend: Tensor, enc_out: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        # Self attention\n",
    "        auto_out = self.auto_correlation(x_seasonal, x_seasonal, x_seasonal)\n",
    "        s1, t1 = self.series_decomp(auto_out + x_seasonal)\n",
    "        trend = x_trend + self.trend_proj1(t1)\n",
    "        \n",
    "        # Cross attention\n",
    "        cross_out = self.cross_correlation(s1, enc_out, enc_out)\n",
    "        s2, t2 = self.series_decomp(cross_out + s1)\n",
    "        trend = trend + self.trend_proj2(t2)\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_out = self.ff(s2)\n",
    "        s3, t3 = self.series_decomp(ff_out + s2)\n",
    "        trend = trend + self.trend_proj3(t3)\n",
    "        \n",
    "        return s3, trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoformerDecoder(nn.Module):\n",
    "    def __init__(self, config:AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.decoders = nn.ModuleList([AutoformerDecoderLayer(config) for l in range(config.n_decoders)])\n",
    "        self.projection = nn.Linear(config.d_model, config.d_model)\n",
    "        self.series_decomp = SeriesDecomp(config)\n",
    "\n",
    "    def forward(self, x: Tensor, enc_out:Tensor) -> Tensor:\n",
    "        # Initialize seasonal and trend components\n",
    "        I, d = x.shape\n",
    "        x_ens, x_ent = self.series_decomp(x[I//2:])\n",
    "        x_des = torch.cat([x_ens, torch.zeros(self.config.pred_len, d)])\n",
    "        x_det = torch.cat([x_ent, x.mean(dim=0).repeat(self.pred_len, 1)])\n",
    "        \n",
    "        # Progressive refinement through decoder layers\n",
    "        for decoder in self.decoders:\n",
    "            x_des, x_det = decoder(x_des, x_det, enc_out)\n",
    "            \n",
    "        # Final prediction combines both components\n",
    "        return self.projection(x_des) + x_det\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoformer(nn.Module):\n",
    "    def __init__(self, config: AutoformerConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Linear(config.d_model, config.d_model)\n",
    "        self.encoder = AutoformerEncoder(config)\n",
    "        self.decoder = AutoformerDecoder(config)\n",
    "        self.output_proj = nn.Linear(config.d_model, 1)\n",
    "        self.series_decomp = SeriesDecomp(config)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Split input sequence\n",
    "        enc_in = x[:, :self.config.seq_len, :]\n",
    "        \n",
    "        # Encoder\n",
    "        enc_out = self.encoder(self.embedding(enc_in))\n",
    "            \n",
    "        # Initialize decoder inputs\n",
    "        I = self.config.seq_len\n",
    "        dec_init = x[:, I//2:I, :]\n",
    "        seasonal, trend = self.series_decomp(dec_init)\n",
    "        \n",
    "        # Pad seasonal and trend components\n",
    "        pad_seasonal = torch.zeros(\n",
    "            (x.size(0), self.config.pred_len, x.size(-1)), \n",
    "            device=x.device\n",
    "        )\n",
    "        pad_trend = x[:, -1:, :].repeat(1, self.config.pred_len, 1)\n",
    "        \n",
    "        seasonal = torch.cat([seasonal, pad_seasonal], dim=1)\n",
    "        trend = torch.cat([trend, pad_trend], dim=1)\n",
    "        \n",
    "        # Decoder\n",
    "        for dec_layer in self.decoder:\n",
    "            seasonal, trend = dec_layer(seasonal, trend, enc_out)\n",
    "            \n",
    "        # Final prediction\n",
    "        seasonal = self.output_proj(seasonal)\n",
    "        return seasonal + trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AutoformerConfig.__init__() got an unexpected keyword argument 'label_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m curr_config \u001b[38;5;241m=\u001b[39m AutoformerConfig(\n\u001b[1;32m      2\u001b[0m     kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,                     \u001b[38;5;66;03m# default kernel size for moving average\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m,                         \u001b[38;5;66;03m# from --seq_len\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     pred_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,                        \u001b[38;5;66;03m# from --pred_len\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     label_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m,                       \u001b[38;5;66;03m# from --label_len\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     n_encoders\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,                       \u001b[38;5;66;03m# from --e_layers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     n_decoders\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,                       \u001b[38;5;66;03m# from --d_layers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,                        \u001b[38;5;66;03m# typical transformer dimension\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,                          \u001b[38;5;66;03m# typical transformer heads\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     d_ff\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,                          \u001b[38;5;66;03m# typical transformer feed-forward dimension\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.0\u001b[39m,                              \u001b[38;5;66;03m# from --factor, controls auto-correlation delay steps\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     enc_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,                           \u001b[38;5;66;03m# no. of input features for encoder \u001b[39;00m\n\u001b[1;32m     13\u001b[0m     dec_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,                           \u001b[38;5;66;03m# no. of input features for decoder\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     c_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,                            \u001b[38;5;66;03m# no. of output features for decoder\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     root_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/ETT-small/\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;66;03m# path to dataset\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETTh1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETTh1_96_24\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETTh1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     des\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     itr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     23\u001b[0m     is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     24\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m,\n\u001b[1;32m     25\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,\n\u001b[1;32m     27\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: AutoformerConfig.__init__() got an unexpected keyword argument 'label_len'"
     ]
    }
   ],
   "source": [
    "curr_config = AutoformerConfig(\n",
    "    kernel_size=25,                     # default kernel size for moving average\n",
    "    seq_len=96,                         # from --seq_len\n",
    "    pred_len=24,                        # from --pred_len\n",
    "    label_len=48,                       # from --label_len\n",
    "    n_encoders=2,                       # from --e_layers\n",
    "    n_decoders=1,                       # from --d_layers\n",
    "    d_model=512,                        # typical transformer dimension\n",
    "    n_heads=8,                          # typical transformer heads\n",
    "    d_ff=2048,                          # typical transformer feed-forward dimension\n",
    "    c=3.0,                              # from --factor, controls auto-correlation delay steps\n",
    "    enc_in=7,                           # no. of input features for encoder \n",
    "    dec_in=7,                           # no. of input features for decoder\n",
    "    c_out=7,                            # no. of output features for decoder\n",
    "    root_path=\"./dataset/ETT-small/\",   # path to dataset\n",
    "    data_path=\"ETTh1.csv\",\n",
    "    model_id=\"ETTh1_96_24\",\n",
    "    model=\"Autoformer\",\n",
    "    data=\"ETTh1\",\n",
    "    features=\"M\",\n",
    "    des=\"Exp\",\n",
    "    itr=1,\n",
    "    is_training=1,\n",
    "    patience=7,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    learning_rate=0.0001,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoformer(curr_config).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(self, batch_x, batch_y, batch_x_mark, batch_y_mark):\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -curr_config.pred_len:, :]).float()\n",
    "    dec_inp = torch.cat([batch_y[:, :curr_config.label_len, :], dec_inp], dim=1).float().to(curr_config.device)\n",
    "    # encoder - decoder\n",
    "\n",
    "    def _run_model():\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        if curr_config.output_attention:\n",
    "            outputs = outputs[0]\n",
    "        return outputs\n",
    "\n",
    "    outputs = _run_model()\n",
    "\n",
    "    f_dim = -1 if self.args.features == 'MS' else 0\n",
    "    outputs = outputs[:, -curr_config.pred_len:, f_dim:]\n",
    "    batch_y = batch_y[:, -curr_config.pred_len:, f_dim:].to(curr_config.device)\n",
    "\n",
    "    return outputs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(vali_data, vali_loader, criterion):\n",
    "    total_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "            batch_x = batch_x.float().to(curr_config.device)\n",
    "            batch_y = batch_y.float()\n",
    "\n",
    "            batch_x_mark = batch_x_mark.float().to(curr_config.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(curr_config.device)\n",
    "\n",
    "            outputs, batch_y = _predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "\n",
    "            pred = outputs.detach().cpu()\n",
    "            true = batch_y.detach().cpu()\n",
    "\n",
    "            loss = criterion(pred, true)\n",
    "\n",
    "            total_loss.append(loss)\n",
    "    total_loss = np.average(total_loss)\n",
    "    model.train()\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_data, train_loader = data_provider(curr_config, flag='train')\n",
    "    vali_data, vali_loader = data_provider(curr_config, flag='val')\n",
    "    test_data, test_loader = data_provider(curr_config, flag='test')\n",
    "\n",
    "    time_now = time.time()\n",
    "    train_steps = len(train_loader)\n",
    "    # early_stopping = EarlyStopping(patience=curr_config.patience, verbose=True)\n",
    "\n",
    "    model_optim = torch.optim.Adam(model.parameters(), lr=curr_config.learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if curr_config.use_amp:\n",
    "        scaler = torch.amp.GradScaler()\n",
    "\n",
    "    for epoch in range(curr_config.train_epochs):\n",
    "        iter_count = 0\n",
    "        train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        epoch_time = time.time()\n",
    "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "            iter_count += 1\n",
    "            model_optim.zero_grad()\n",
    "            batch_x = batch_x.float().to(curr_config.device)\n",
    "\n",
    "            batch_y = batch_y.float().to(curr_config.device)\n",
    "            batch_x_mark = batch_x_mark.float().to(curr_config.device)\n",
    "            batch_y_mark = batch_y_mark.float().to(curr_config.device)\n",
    "\n",
    "            outputs, batch_y = _predict(batch_x, batch_y, batch_x_mark, batch_y_mark)\n",
    "\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                speed = (time.time() - time_now) / iter_count\n",
    "                left_time = speed * ((curr_config.train_epochs - epoch) * train_steps - i)\n",
    "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                iter_count = 0\n",
    "                time_now = time.time()\n",
    "\n",
    "            if curr_config.use_amp:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(model_optim)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                model_optim.step()\n",
    "\n",
    "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "        train_loss = np.average(train_loss)\n",
    "        vali_loss = vali(vali_data, vali_loader, criterion)\n",
    "        test_loss = vali(test_data, test_loader, criterion)\n",
    "\n",
    "        print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "            epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "        # early_stopping(vali_loss, model, path)\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"Early stopping\")\n",
    "        #     break\n",
    "\n",
    "        # adjust_learning_rate(model_optim, epoch + 1, curr_config)\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
